{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e0f4e4",
   "metadata": {},
   "source": [
    "# BKF Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82898195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "#   BKF e-Flora Thailand Crawler (Vol. 2‚Äì16)\n",
    "#   ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Accepted Name / Thailand / Distribution / Ecology\n",
    "#   Author: ChatGPT (GPT-5) | Updated: 2025-10-16\n",
    "# ==========================================================\n",
    "# !pip install selenium webdriver-manager pandas beautifulsoup4\n",
    "\n",
    "import re, time, random, requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ---------- SETTINGS ----------\n",
    "BASE = \"https://botany.dnp.go.th/eflora/\"\n",
    "VOLUMES = range(2, 17)           # Volume 2‚Äì16\n",
    "OUT_CSV = \"bkf_eflora_species_vol2_16.csv\"\n",
    "SLEEP_BASE = 1.2                 # ‡∏û‡∏±‡∏Å‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á request\n",
    "WAIT_SEC = 20                    # Selenium explicit wait (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)\n",
    "HEADLESS = True                  # ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô False ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏î‡∏π browser ‡∏à‡∏£‡∏¥‡∏á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e272074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Resume mode: ‡πÇ‡∏´‡∏•‡∏î 80 ‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏¥‡∏°, species ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÅ‡∏•‡πâ‡∏ß 22\n",
      "\n",
      "=== Volume 2 ===\n",
      "  ‚Üí ‡∏û‡∏ö 46 families (‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á‡∏ã‡πâ‡∏≥)\n",
      "    [1/46] Haloragaceae: 2 genera\n",
      "    [2/46] Rhizophoraceae: 7 genera\n",
      "    [3/46] Oxalidaceae: 3 genera\n",
      "    [4/46] Ochnaceae: 4 genera\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hb/q_224lns0mlcx0n865_2ylqc0000gn/T/ipykernel_73306/3377219641.py:290: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"scraped_at\": datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [5/46] Rosaceae: 21 genera\n",
      "    [6/46] Icacinaceae: 13 genera\n",
      "    [7/46] Cardiopteridaceae: 1 genera\n",
      "    [8/46] Dilleniaceae: 3 genera\n",
      "    [9/46] Saurauiaceae: 1 genera\n",
      "    [10/46] Schisandraceae: 1 genera\n",
      "    [11/46] Illiciaceae: 1 genera\n",
      "    [12/46] Connaraceae: 6 genera\n",
      "    [13/46] Apostasiaceae: 2 genera\n",
      "    [14/46] Actinidiaceae: 2 genera\n",
      "    [15/46] Theaceae: 10 genera\n",
      "    [16/46] Bonnetiaceae: 1 genera\n",
      "    [17/46] Centrolepidaceae: 1 genera\n",
      "    [18/46] Flagellariaceae: 1 genera\n",
      "    [19/46] Hanguanaceae: 1 genera\n",
      "    [20/46] Juncaceae: 1 genera\n",
      "    [21/46] Lowiaceae: 1 genera\n",
      "    [22/46] Restionaceae: 1 genera\n",
      "    [23/46] Triuridaceae: 1 genera\n",
      "    [24/46] Balanophoraceae: 1 genera\n",
      "    [25/46] Rafflesiaceae: 2 genera\n",
      "    [26/46] Cycadaceae: 1 genera\n",
      "    [27/46] Pinaceae: 1 genera\n",
      "    [28/46] Cephalotaxaceae: 1 genera\n",
      "    [29/46] Cupressaceae: 1 genera\n",
      "    [30/46] Podocarpaceae: 2 genera\n",
      "    [31/46] Gnetaceae: 1 genera\n",
      "    [32/46] Smilacaceae: 2 genera\n",
      "    [33/46] Magnoliaceae: 7 genera\n",
      "    [34/46] Portulacaceae: 2 genera\n",
      "    [35/46] Stylidaceae: 1 genera\n",
      "    [36/46] Goodeniaceae: 2 genera\n",
      "    [37/46] Sphenocleaceae: 1 genera\n",
      "    [38/46] Ebenaceae: 1 genera\n",
      "    [39/46] Cannabidaceae: 1 genera\n",
      "    [40/46] Hippocastanaceae: 1 genera\n",
      "    [41/46] Irvingiaceae: 1 genera\n",
      "    [42/46] Casuarinaceae: 1 genera\n",
      "    [43/46] Nyssaceae: 1 genera\n",
      "    [44/46] Elaeocarpaceae: 2 genera\n",
      "    [45/46] Simaroubaceae: 5 genera\n",
      "    [46/46] Symplocaceae: 1 genera\n",
      "\n",
      "‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: Saved 310 rows ‚Üí bkf_eflora_species_vol2_16.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BKF e-Flora Thailand Crawler (Vol. 2‚Äì16, Full + Resume)\n",
    "# - Family discovery: a[href^='florafamily.html'] (dedup)\n",
    "# - Family name: from URL ?factsheet=<FAMILY> (robust) + fallback <p class=\"intro\">Family :</p>\n",
    "# - Click \"List of lower taxa\" tab before reading Genus\n",
    "# - Drilldown: Family -> Genus -> Species\n",
    "# - Extract: Accepted Name / Thailand / Distribution / Ecology\n",
    "# - Sleep: 1‚Äì2s (randomized) to be polite\n",
    "# - Resume: skip species_url already saved in CSV\n",
    "# Updated: 2025-10-16\n",
    "# ==========================================================\n",
    "\n",
    "import os, re, time, random, requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ---------- SETTINGS ----------\n",
    "BASE        = \"https://botany.dnp.go.th/eflora/\"\n",
    "VOLUMES     = range(2, 3)                 # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏•‡πà‡∏°‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô range(2,3) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏™ Vol.2 ‡∏Å‡πà‡∏≠‡∏ô\n",
    "OUT_CSV     = \"bkf_eflora_species_vol2_16.csv\"\n",
    "SLEEP_BASE  = 1.2                          # ‡∏û‡∏±‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡∏∏‡πà‡∏°‡∏Ñ‡∏π‡∏ì‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢)\n",
    "WAIT_SEC    = 25                           # Explicit wait ‡∏Ç‡∏≠‡∏á Selenium\n",
    "HEADLESS    = True                         # False ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏´‡πá‡∏ô‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏à‡∏£‡∏¥‡∏á\n",
    "\n",
    "# ---------- UTILITIES ----------\n",
    "def nap(mult: float = 1.0):\n",
    "    \"\"\"Sleep ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏° (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 1‚Äì2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)\"\"\"\n",
    "    time.sleep(SLEEP_BASE * mult * (0.85 + random.random()*0.3))\n",
    "\n",
    "def clean(t: str | None):\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip() if t else None\n",
    "\n",
    "def to_abs(url: str | None):\n",
    "    \"\"\"‡πÅ‡∏õ‡∏•‡∏á relative URL -> absolute URL\"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "    if url.startswith(\"http\"):\n",
    "        return url\n",
    "    return requests.compat.urljoin(BASE, url)\n",
    "\n",
    "def soup_from_driver(driver):\n",
    "    return BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "def find_label_value(soup: BeautifulSoup, label_regex: str):\n",
    "    \"\"\"\n",
    "    ‡∏´‡∏≤ value ‡∏ó‡∏µ‡πà‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏á label (Accepted Name / Distribution / Ecology / Thailand) ‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô\n",
    "    \"\"\"\n",
    "    tag = soup.find(string=re.compile(label_regex, re.I))\n",
    "    if tag:\n",
    "        cur = tag\n",
    "        for _ in range(12):\n",
    "            cur = cur.next_element\n",
    "            if cur is None:\n",
    "                break\n",
    "            txt = clean(cur.get_text(\" \", strip=True) if hasattr(cur, \"get_text\") else str(cur))\n",
    "            if txt and not re.search(label_regex, txt, re.I):\n",
    "                return re.sub(r\"^[:\\s]+\", \"\", txt)\n",
    "    # fallback: grep ‡∏à‡∏≤‡∏Å text ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏ô‡πâ‡∏≤\n",
    "    full = soup.get_text(\"\\n\", strip=True)\n",
    "    m = re.search(rf\"{label_regex}\\s*:?\\s*(.+)\", full, re.I)\n",
    "    return clean(m.group(1)) if m else None\n",
    "\n",
    "def setup_driver(headless: bool = True):\n",
    "    \"\"\"‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Selenium Chrome\"\"\"\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--window-size=1280,900\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--user-agent=Mozilla/5.0\")\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    return driver\n",
    "\n",
    "# ---------- FAMILY DISCOVERY (robust) ----------\n",
    "def get_families_for_volume(driver, vol: int):\n",
    "    \"\"\"\n",
    "    ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡πâ‡∏≤ Volume -> ‡∏î‡∏∂‡∏á family ‡∏à‡∏≤‡∏Å a[href^='florafamily.html'] (dedup)\n",
    "    ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ family ‡∏à‡∏≤‡∏Å‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå ?factsheet= ‡πÉ‡∏ô URL ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å\n",
    "    ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ‡∏Ñ‡πà‡∏≠‡∏¢ fallback ‡πÑ‡∏õ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å <p class=\"intro\"><strong>Family :</strong> ...</p> ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤ family\n",
    "    \"\"\"\n",
    "    vol_url = f\"{BASE}floramainvol.html?vol={vol}\"\n",
    "    driver.get(vol_url)\n",
    "    WebDriverWait(driver, WAIT_SEC).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    time.sleep(3)  # ‡∏£‡∏≠ JS render ‡∏ï‡∏≤‡∏£‡∏≤‡∏á\n",
    "\n",
    "    soup = soup_from_driver(driver)\n",
    "    families, seen = [], set()\n",
    "\n",
    "    for a in soup.select(\"a[href^='florafamily.html']\"):\n",
    "        href = to_abs(a.get(\"href\", \"\"))\n",
    "        if not href or href in seen:\n",
    "            continue\n",
    "        seen.add(href)\n",
    "\n",
    "        # 1) ‡∏ä‡∏∑‡πà‡∏≠‡∏à‡∏≤‡∏Å ?factsheet=\n",
    "        fam_name = None\n",
    "        try:\n",
    "            qs = parse_qs(urlparse(href).query)\n",
    "            fam_name = qs.get(\"factsheet\", [None])[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 2) ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠ ‡∏•‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ family\n",
    "        if not fam_name:\n",
    "            driver.get(href)\n",
    "            try:\n",
    "                WebDriverWait(driver, WAIT_SEC).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"p.intro\")))\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "            s = soup_from_driver(driver)\n",
    "            p = s.select_one(\"p.intro\")\n",
    "            if p and \"Family\" in p.get_text():\n",
    "                txt = p.get_text(\" \", strip=True)\n",
    "                fam_name = re.sub(r\".*Family\\s*:\\s*\", \"\", txt)\n",
    "\n",
    "        families.append((clean(fam_name) if fam_name else None, href))\n",
    "\n",
    "    return families\n",
    "\n",
    "# ---------- GENUS & SPECIES ----------\n",
    "def get_genus_links(driver, family_url: str):\n",
    "    \"\"\"\n",
    "    ‡πÄ‡∏Ç‡πâ‡∏≤ family ‡πÅ‡∏•‡πâ‡∏ß '‡∏Ñ‡∏•‡∏¥‡∏Å‡πÅ‡∏ó‡πá‡∏ö List of lower taxa' ‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡πà‡∏≠‡∏¢‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå Genus\n",
    "    \"\"\"\n",
    "    driver.get(to_abs(family_url))\n",
    "\n",
    "    # ‡∏£‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î\n",
    "    try:\n",
    "        WebDriverWait(driver, WAIT_SEC).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    except TimeoutException:\n",
    "        time.sleep(2)\n",
    "\n",
    "    # ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÅ‡∏ó‡πá‡∏ö 'List of lower taxa' (‡∏ï‡∏±‡∏ß‡∏™‡∏∞‡∏Å‡∏î/‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏≤‡∏à‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÉ‡∏ô‡∏ö‡∏≤‡∏á Family)\n",
    "    clicked = False\n",
    "    for locator in [\n",
    "        (By.XPATH, \"//a[contains(., 'List of lower taxa')]\"),\n",
    "        (By.XPATH, \"//li[contains(., 'List of lower taxa')]//a\"),\n",
    "        (By.CSS_SELECTOR, \"a[href*='lower'], a[href*='#lower'], li a[href*='lower']\")\n",
    "    ]:\n",
    "        try:\n",
    "            el = WebDriverWait(driver, 5).until(EC.element_to_be_clickable(locator))\n",
    "            el.click()\n",
    "            clicked = True\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not clicked:\n",
    "        time.sleep(1)\n",
    "\n",
    "    # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏•‡∏¥‡∏á‡∏Å‡πå Genus ‡∏õ‡∏£‡∏≤‡∏Å‡∏è\n",
    "    try:\n",
    "        WebDriverWait(driver, WAIT_SEC).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href*='floragenus.html']\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        time.sleep(2)\n",
    "\n",
    "    s = soup_from_driver(driver)\n",
    "    gens = []\n",
    "    for a in s.select(\"a[href*='floragenus.html']\"):\n",
    "        gens.append((clean(a.get_text()), to_abs(a.get(\"href\", \"\"))))\n",
    "\n",
    "    # ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏à‡∏≤‡∏Å DOM (‡∏Å‡∏±‡∏ô soup ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö)\n",
    "    if not gens:\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, \"a[href*='floragenus.html']\")\n",
    "        gens = [(clean(e.text), to_abs(e.get_attribute(\"href\"))) for e in elems]\n",
    "\n",
    "    # unique\n",
    "    uniq, seen = [], set()\n",
    "    for name, url in gens:\n",
    "        if url and url not in seen:\n",
    "            seen.add(url)\n",
    "            uniq.append((name, url))\n",
    "    return uniq\n",
    "\n",
    "def get_species_links(driver, genus_url: str):\n",
    "    driver.get(to_abs(genus_url))\n",
    "    try:\n",
    "        WebDriverWait(driver, WAIT_SEC).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href*='floraspecies.html']\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        time.sleep(2)\n",
    "    s = soup_from_driver(driver)\n",
    "    sp = [to_abs(a.get(\"href\", \"\")) for a in s.select(\"a[href*='floraspecies.html']\")]\n",
    "    if not sp:\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, \"a[href*='floraspecies.html']\")\n",
    "        sp = [to_abs(e.get_attribute(\"href\")) for e in elems]\n",
    "    # unique\n",
    "    out, seen = [], set()\n",
    "    for u in sp:\n",
    "        if u and u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "def parse_species_page(driver, sp_url: str):\n",
    "    driver.get(to_abs(sp_url))\n",
    "    try:\n",
    "        WebDriverWait(driver, WAIT_SEC).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    s = soup_from_driver(driver)\n",
    "\n",
    "    accepted     = find_label_value(s, r\"Accepted\\s*Name\")\n",
    "    thailand     = find_label_value(s, r\"Thailand\")\n",
    "    distribution = find_label_value(s, r\"Distribution\")\n",
    "    ecology      = find_label_value(s, r\"Ecology\")\n",
    "\n",
    "    sci_name = None\n",
    "    if accepted:\n",
    "        m = re.match(r\"([A-Z][a-zA-Z-]+)\\s+([a-z\\-]+)\", accepted)\n",
    "        if m:\n",
    "            sci_name = f\"{m.group(1)} {m.group(2)}\"\n",
    "\n",
    "    return {\n",
    "        \"accepted_name\": clean(accepted),\n",
    "        \"species_scientific_name\": clean(sci_name),\n",
    "        \"thailand\": clean(thailand),\n",
    "        \"distribution\": clean(distribution),\n",
    "        \"ecology\": clean(ecology),\n",
    "    }\n",
    "\n",
    "# ---------- MAIN (with Resume) ----------\n",
    "def crawl_all(volumes=VOLUMES, out_csv=OUT_CSV):\n",
    "    # Resume: ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°\n",
    "    rows, done_species = [], set()\n",
    "    if os.path.exists(out_csv):\n",
    "        try:\n",
    "            old = pd.read_csv(out_csv)\n",
    "            rows = old.to_dict(\"records\")\n",
    "            if \"species_url\" in old.columns:\n",
    "                done_species = set(old[\"species_url\"].dropna().astype(str).tolist())\n",
    "            print(f\"üß© Resume mode: ‡πÇ‡∏´‡∏•‡∏î {len(rows)} ‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏¥‡∏°, species ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÅ‡∏•‡πâ‡∏ß {len(done_species)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}\")\n",
    "\n",
    "    driver = setup_driver(headless=HEADLESS)\n",
    "\n",
    "    try:\n",
    "        for vol in volumes:\n",
    "            families = get_families_for_volume(driver, vol)\n",
    "            print(f\"\\n=== Volume {vol} ===\")\n",
    "            print(f\"  ‚Üí ‡∏û‡∏ö {len(families)} families (‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á‡∏ã‡πâ‡∏≥)\")\n",
    "\n",
    "            for fi, (family_name, family_url) in enumerate(families, 1):\n",
    "                family_name = clean(family_name) or \"Unknown Family\"\n",
    "                nap()\n",
    "                genus_links = get_genus_links(driver, family_url)\n",
    "                print(f\"    [{fi}/{len(families)}] {family_name}: {len(genus_links)} genera\")\n",
    "\n",
    "                for gi, (genus_name, genus_url) in enumerate(genus_links, 1):\n",
    "                    nap()\n",
    "                    species_links = get_species_links(driver, genus_url)\n",
    "\n",
    "                    for si, sp_url in enumerate(species_links, 1):\n",
    "                        if not sp_url or sp_url in done_species:\n",
    "                            continue\n",
    "                        nap(1.1)\n",
    "                        try:\n",
    "                            data = parse_species_page(driver, sp_url)\n",
    "                        except Exception:\n",
    "                            data = {\n",
    "                                \"accepted_name\": None,\n",
    "                                \"species_scientific_name\": None,\n",
    "                                \"thailand\": None,\n",
    "                                \"distribution\": None,\n",
    "                                \"ecology\": None,\n",
    "                            }\n",
    "\n",
    "                        rows.append({\n",
    "                            \"volume\": vol,\n",
    "                            \"family_name\": family_name,\n",
    "                            \"genus_name\": clean(genus_name),\n",
    "                            **data,\n",
    "                            \"family_url\": to_abs(family_url),\n",
    "                            \"genus_url\": to_abs(genus_url),\n",
    "                            \"species_url\": to_abs(sp_url),\n",
    "                            \"scraped_at\": datetime.utcnow().isoformat()\n",
    "                        })\n",
    "                        done_species.add(sp_url)\n",
    "\n",
    "                        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏û‡∏±‡∏Å‡∏ó‡∏∏‡∏Å 50 ‡πÅ‡∏ñ‡∏ß\n",
    "                        if len(rows) % 50 == 0:\n",
    "                            pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "\n",
    "            # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏à‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ volume\n",
    "            pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "\n",
    "        # Final write + dedup\n",
    "        df = pd.DataFrame(rows)\n",
    "        if not df.empty:\n",
    "            df = df.drop_duplicates(subset=[\"species_url\"]).reset_index(drop=True)\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        print(f\"\\n‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: Saved {len(df)} rows ‚Üí {out_csv}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# ---------- RUN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ VOLUMES = range(2,3) ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏Ç‡∏¢‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô range(2,17)\n",
    "    crawl_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
