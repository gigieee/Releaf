{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90fe94e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Resume mode: ‡πÇ‡∏´‡∏•‡∏î 5700 ‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏¥‡∏°, species ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÅ‡∏•‡πâ‡∏ß 5700\n",
      "\n",
      "=== Volume 16 ===\n",
      "  ‚Üí ‡∏û‡∏ö 2 families (‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á‡∏ã‡πâ‡∏≥)\n",
      "    [1/2] Zingiberaceae: 28 genera\n",
      "    [2/2] Annonaceae: 39 genera\n",
      "\n",
      "‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: Saved 6354 rows ‚Üí bkf_eflora_species_vol2_16.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# BKF e-Flora Thailand Crawler (Vol. 2‚Äì16, Full + Resume)\n",
    "# - Family discovery: a[href^='florafamily.html'] (dedup)\n",
    "# - Family name: from URL ?factsheet=<FAMILY> (robust) + fallback <p class=\"intro\">Family :</p>\n",
    "# - Click \"List of lower taxa\" tab before reading Genus\n",
    "# - Drilldown: Family -> Genus(link text in List of lower taxa) -> Species\n",
    "# - Extract:\n",
    "#     species_scientific_name = <p class=\"intro\"><strong>...</strong></p> (‡∏´‡∏ô‡πâ‡∏≤ species)\n",
    "#     accepted_name / Thailand / Distribution / Ecology ‡∏à‡∏≤‡∏Å label ‡πÄ‡∏î‡∏¥‡∏°\n",
    "# - Sleep: 1‚Äì2s (randomized)\n",
    "# - Resume: skip species_url ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô CSV\n",
    "# Updated: 2025-10-17\n",
    "# ==========================================================\n",
    "\n",
    "import os, re, time, random, requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, UTC\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ---------- SETTINGS ----------\n",
    "BASE        = \"https://botany.dnp.go.th/eflora/\"\n",
    "VOLUMES     = range(16, 17)                 # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏•‡πà‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà (‡∏ó‡∏î‡∏™‡∏≠‡∏ö: range(2,3))\n",
    "OUT_CSV     = \"bkf_eflora_species_vol2_16.csv\"\n",
    "SLEEP_BASE  = 1.2\n",
    "WAIT_SEC    = 25\n",
    "HEADLESS    = True\n",
    "\n",
    "# ---------- UTILITIES ----------\n",
    "def nap(mult: float = 1.0):\n",
    "    time.sleep(SLEEP_BASE * mult * (0.85 + random.random()*0.3))\n",
    "\n",
    "def clean(t: str | None):\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip() if t else None\n",
    "\n",
    "def to_abs(url: str | None):\n",
    "    if not url:\n",
    "        return None\n",
    "    if url.startswith(\"http\"):\n",
    "        return url\n",
    "    return requests.compat.urljoin(BASE, url)\n",
    "\n",
    "def soup_from_driver(driver):\n",
    "    return BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "def find_label_value(soup: BeautifulSoup, label_regex: str):\n",
    "    tag = soup.find(string=re.compile(label_regex, re.I))\n",
    "    if tag:\n",
    "        cur = tag\n",
    "        for _ in range(12):\n",
    "            cur = cur.next_element\n",
    "            if cur is None:\n",
    "                break\n",
    "            txt = clean(cur.get_text(\" \", strip=True) if hasattr(cur, \"get_text\") else str(cur))\n",
    "            if txt and not re.search(label_regex, txt, re.I):\n",
    "                return re.sub(r\"^[:\\s]+\", \"\", txt)\n",
    "    full = soup.get_text(\"\\n\", strip=True)\n",
    "    m = re.search(rf\"{label_regex}\\s*:?\\s*(.+)\", full, re.I)\n",
    "    return clean(m.group(1)) if m else None\n",
    "\n",
    "def setup_driver(headless: bool = True):\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--window-size=1280,900\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--user-agent=Mozilla/5.0\")\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    return driver\n",
    "\n",
    "# ---------- FAMILY DISCOVERY (robust) ----------\n",
    "def get_families_for_volume(driver, vol: int):\n",
    "    vol_url = f\"{BASE}floramainvol.html?vol={vol}\"\n",
    "    driver.get(vol_url)\n",
    "    WebDriverWait(driver, WAIT_SEC).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = soup_from_driver(driver)\n",
    "    families, seen = [], set()\n",
    "\n",
    "    for a in soup.select(\"a[href^='florafamily.html']\"):\n",
    "        href = to_abs(a.get(\"href\", \"\"))\n",
    "        if not href or href in seen:\n",
    "            continue\n",
    "        seen.add(href)\n",
    "\n",
    "        fam_name = None\n",
    "        try:\n",
    "            qs = parse_qs(urlparse(href).query)\n",
    "            fam_name = qs.get(\"factsheet\", [None])[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not fam_name:\n",
    "            driver.get(href)\n",
    "            try:\n",
    "                WebDriverWait(driver, WAIT_SEC).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"p.intro\")))\n",
    "            except TimeoutException:\n",
    "                pass\n",
    "            s = soup_from_driver(driver)\n",
    "            p = s.select_one(\"p.intro\")\n",
    "            if p and \"Family\" in p.get_text():\n",
    "                txt = p.get_text(\" \", strip=True)\n",
    "                fam_name = re.sub(r\".*Family\\s*:\\s*\", \"\", txt)\n",
    "\n",
    "        families.append((clean(fam_name) if fam_name else None, href))\n",
    "\n",
    "    return families\n",
    "\n",
    "# ---------- GENUS & SPECIES ----------\n",
    "def get_genus_links(driver, family_url: str):\n",
    "    \"\"\"\n",
    "    ‡πÄ‡∏Ç‡πâ‡∏≤ family ‡πÅ‡∏•‡πâ‡∏ß '‡∏Ñ‡∏•‡∏¥‡∏Å‡πÅ‡∏ó‡πá‡∏ö List of lower taxa' ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå Genus\n",
    "    genus_name = ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ö‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ô‡πÅ‡∏ó‡πá‡∏ö‡∏ô‡∏µ‡πâ (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)\n",
    "    \"\"\"\n",
    "    driver.get(to_abs(family_url))\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, WAIT_SEC).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    except TimeoutException:\n",
    "        time.sleep(2)\n",
    "\n",
    "    # ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÅ‡∏ó‡πá‡∏ö 'List of lower taxa'\n",
    "    clicked = False\n",
    "    for locator in [\n",
    "        (By.XPATH, \"//a[contains(., 'List of lower taxa')]\"),\n",
    "        (By.XPATH, \"//li[contains(., 'List of lower taxa')]//a\"),\n",
    "        (By.CSS_SELECTOR, \"a[href*='lower'], a[href*='#lower'], li a[href*='lower']\")\n",
    "    ]:\n",
    "        try:\n",
    "            el = WebDriverWait(driver, 5).until(EC.element_to_be_clickable(locator))\n",
    "            el.click()\n",
    "            clicked = True\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not clicked:\n",
    "        time.sleep(1)\n",
    "\n",
    "    # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏•‡∏¥‡∏á‡∏Å‡πå Genus ‡πÇ‡∏ú‡∏•‡πà\n",
    "    try:\n",
    "        WebDriverWait(driver, WAIT_SEC).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href*='floragenus.html']\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        time.sleep(2)\n",
    "\n",
    "    s = soup_from_driver(driver)\n",
    "    gens = []\n",
    "    for a in s.select(\"div#datalower a[href*='floragenus.html'], a[href*='floragenus.html']\"):\n",
    "        gens.append((clean(a.get_text()), to_abs(a.get(\"href\", \"\"))))\n",
    "\n",
    "    if not gens:\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, \"div#datalower a[href*='floragenus.html'], a[href*='floragenus.html']\")\n",
    "        gens = [(clean(e.text), to_abs(e.get_attribute(\"href\"))) for e in elems]\n",
    "\n",
    "    uniq, seen = [], set()\n",
    "    for name, url in gens:\n",
    "        if url and url not in seen:\n",
    "            seen.add(url)\n",
    "            uniq.append((name, url))\n",
    "    return uniq\n",
    "\n",
    "def get_species_links(driver, genus_url: str):\n",
    "    driver.get(to_abs(genus_url))\n",
    "    try:\n",
    "        WebDriverWait(driver, WAIT_SEC).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href*='floraspecies.html']\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        time.sleep(2)\n",
    "    s = soup_from_driver(driver)\n",
    "    sp = [to_abs(a.get(\"href\", \"\")) for a in s.select(\"a[href*='floraspecies.html']\")]\n",
    "    if not sp:\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, \"a[href*='floraspecies.html']\")\n",
    "        sp = [to_abs(e.get_attribute(\"href\")) for e in elems]\n",
    "    out, seen = [], set()\n",
    "    for u in sp:\n",
    "        if u and u not in seen:\n",
    "            seen.add(u)\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "def parse_species_page(driver, sp_url: str):\n",
    "    \"\"\"\n",
    "    species_scientific_name = ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô <p class=\"intro\"><strong>...</strong></p> (‡∏ï‡∏±‡∏î‡πÄ‡∏•‡∏Ç‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤)\n",
    "    accepted_name / thailand / distribution / ecology = ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡πà‡∏≤‡∏ô label ‡πÄ‡∏î‡∏¥‡∏°\n",
    "    \"\"\"\n",
    "    driver.get(to_abs(sp_url))\n",
    "    try:\n",
    "        WebDriverWait(driver, WAIT_SEC).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    s = soup_from_driver(driver)\n",
    "\n",
    "    # --- species_scientific_name ‡∏à‡∏≤‡∏Å‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ strong ---\n",
    "    species_heading = None\n",
    "    strong = s.select_one(\"p.intro > strong\")\n",
    "    if strong:\n",
    "        species_heading = clean(strong.get_text())\n",
    "        # ‡∏ï‡∏±‡∏î‡πÄ‡∏•‡∏Ç‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ô‡∏≥‡∏´‡∏ô‡πâ‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô \"2. \"\n",
    "        species_heading = re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", species_heading or \"\")\n",
    "\n",
    "    # --- fields ‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏° ---\n",
    "    accepted     = find_label_value(s, r\"Accepted\\s*Name\")\n",
    "    thailand     = find_label_value(s, r\"Thailand\")\n",
    "    distribution = find_label_value(s, r\"Distribution\")\n",
    "    ecology      = find_label_value(s, r\"Ecology\")\n",
    "\n",
    "    return {\n",
    "        \"species_scientific_name\": clean(species_heading),\n",
    "        \"accepted_name\": clean(accepted),\n",
    "        \"thailand\": clean(thailand),\n",
    "        \"distribution\": clean(distribution),\n",
    "        \"ecology\": clean(ecology),\n",
    "    }\n",
    "\n",
    "# ---------- MAIN (with Resume) ----------\n",
    "def crawl_all(volumes=VOLUMES, out_csv=OUT_CSV):\n",
    "    rows, done_species = [], set()\n",
    "    if os.path.exists(out_csv):\n",
    "        try:\n",
    "            old = pd.read_csv(out_csv)\n",
    "            rows = old.to_dict(\"records\")\n",
    "            if \"species_url\" in old.columns:\n",
    "                done_species = set(old[\"species_url\"].dropna().astype(str).tolist())\n",
    "            print(f\"üß© Resume mode: ‡πÇ‡∏´‡∏•‡∏î {len(rows)} ‡πÅ‡∏ñ‡∏ß‡πÄ‡∏î‡∏¥‡∏°, species ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÅ‡∏•‡πâ‡∏ß {len(done_species)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}\")\n",
    "\n",
    "    driver = setup_driver(headless=HEADLESS)\n",
    "\n",
    "    try:\n",
    "        for vol in volumes:\n",
    "            families = get_families_for_volume(driver, vol)\n",
    "            print(f\"\\n=== Volume {vol} ===\")\n",
    "            print(f\"  ‚Üí ‡∏û‡∏ö {len(families)} families (‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á‡∏ã‡πâ‡∏≥)\")\n",
    "\n",
    "            for fi, (family_name, family_url) in enumerate(families, 1):\n",
    "                family_name = clean(family_name) or \"Unknown Family\"\n",
    "                nap()\n",
    "                genus_links = get_genus_links(driver, family_url)\n",
    "                print(f\"    [{fi}/{len(families)}] {family_name}: {len(genus_links)} genera\")\n",
    "\n",
    "                for gi, (genus_name, genus_url) in enumerate(genus_links, 1):\n",
    "                    # genus_name = ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ö‡∏ô‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ô List of lower taxa (‡∏ï‡∏≤‡∏° requirement ‡πÉ‡∏´‡∏°‡πà)\n",
    "                    genus_name = clean(genus_name)\n",
    "                    nap()\n",
    "                    species_links = get_species_links(driver, genus_url)\n",
    "\n",
    "                    for si, sp_url in enumerate(species_links, 1):\n",
    "                        if not sp_url or sp_url in done_species:\n",
    "                            continue\n",
    "                        nap(1.1)\n",
    "                        try:\n",
    "                            data = parse_species_page(driver, sp_url)\n",
    "                        except Exception:\n",
    "                            data = {\n",
    "                                \"species_scientific_name\": None,\n",
    "                                \"accepted_name\": None,\n",
    "                                \"thailand\": None,\n",
    "                                \"distribution\": None,\n",
    "                                \"ecology\": None,\n",
    "                            }\n",
    "\n",
    "                        rows.append({\n",
    "                            \"volume\": vol,\n",
    "                            \"family_name\": family_name,\n",
    "                            \"genus_name\": genus_name,  # ‚Üê ‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏ô List of lower taxa\n",
    "                            **data,\n",
    "                            \"family_url\": to_abs(family_url),\n",
    "                            \"genus_url\": to_abs(genus_url),\n",
    "                            \"species_url\": to_abs(sp_url),\n",
    "                            \"scraped_at\": datetime.now(UTC).isoformat()\n",
    "                        })\n",
    "                        done_species.add(sp_url)\n",
    "\n",
    "                        if len(rows) % 50 == 0:\n",
    "                            pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "\n",
    "            pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "        if not df.empty:\n",
    "            df = df.drop_duplicates(subset=[\"species_url\"]).reset_index(drop=True)\n",
    "        df.to_csv(out_csv, index=False)\n",
    "        print(f\"\\n‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: Saved {len(df)} rows ‚Üí {out_csv}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# ---------- RUN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
