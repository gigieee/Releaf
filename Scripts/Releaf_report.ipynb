{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275bd1c3",
   "metadata": {},
   "source": [
    "\n",
    "# üå± ReLeaf ‚Äì BKF √ó WFO Mapping (Structured Outputs)\n",
    "\n",
    "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏£‡∏ß‡∏° **‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î/‡πÅ‡∏°‡∏õ WFO** ‡∏â‡∏ö‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ **‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö** ‡∏•‡∏á‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå `Processed data` \n",
    "‡∏û‡∏£‡πâ‡∏≠‡∏° Markdown ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏≠‡∏≠‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5acfa",
   "metadata": {},
   "source": [
    "\n",
    "## üóÇÔ∏è ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ `Processed data/`)\n",
    "\n",
    "```\n",
    "Processed data/\n",
    "‚îú‚îÄ 00_logs/                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å/‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° log ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
    "‚îú‚îÄ 01_enriched/            # ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏´‡∏•‡∏±‡∏á enrich/mapping\n",
    "‚îú‚îÄ 02_audit/               # ‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö/‡∏Ç‡∏±‡∏î‡πÅ‡∏¢‡πâ‡∏á (conflicts & diagnostics)\n",
    "‚îú‚îÄ 03_summary/             # ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏™‡∏£‡∏∏‡∏õ/metrics (JSON/CSV)\n",
    "‚îî‚îÄ runs/YYYYMMDD_HHMM/     # ‡∏™‡∏≥‡πÄ‡∏ô‡∏≤ snapshot ‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏∏‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ====== Imports ======\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re, json, shutil\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# ====== CONFIG ======\n",
    "# Root project folder\n",
    "ROOT_DIR = Path(\"/Users/thanwaratkeratipasuwat/Desktop/dsi314\")\n",
    "\n",
    "# Input (Raw data)\n",
    "RAW_DIR  = ROOT_DIR / \"Raw data\"\n",
    "BKF_PATH  = RAW_DIR / \"bkf_eflora_species_dedup.csv\"   # BKF ‡∏´‡∏•‡∏±‡∏á dedup\n",
    "OUT7_PATH = RAW_DIR / \"output (7).csv\"                 # ‡∏à‡∏≤‡∏Å WFO map tools\n",
    "CAND_PATH = RAW_DIR / \"candidates (3).csv\"             # candidates ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
    "\n",
    "# Outputs base\n",
    "OUT_DIR   = ROOT_DIR / \"Processed data\"\n",
    "\n",
    "# Organized output dirs\n",
    "LOG_DIR     = OUT_DIR / \"00_logs\"\n",
    "ENRICH_DIR  = OUT_DIR / \"01_enriched\"\n",
    "AUDIT_DIR   = OUT_DIR / \"02_audit\"\n",
    "SUMMARY_DIR = OUT_DIR / \"03_summary\"\n",
    "RUN_STAMP   = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "RUN_DIR     = OUT_DIR / \"runs\" / RUN_STAMP\n",
    "\n",
    "for p in [OUT_DIR, LOG_DIR, ENRICH_DIR, AUDIT_DIR, SUMMARY_DIR, RUN_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Paths Configuration:\")\n",
    "print(f\"BKF_PATH         : {BKF_PATH}\")\n",
    "print(f\"OUT7_PATH        : {OUT7_PATH}\")\n",
    "print(f\"CAND_PATH        : {CAND_PATH}\")\n",
    "print(f\"OUT_DIR          : {OUT_DIR}\")\n",
    "print(f\"ENRICH_DIR       : {ENRICH_DIR}\")\n",
    "print(f\"AUDIT_DIR        : {AUDIT_DIR}\")\n",
    "print(f\"SUMMARY_DIR      : {SUMMARY_DIR}\")\n",
    "print(f\"RUN_DIR          : {RUN_DIR}\")\n",
    "\n",
    "# ====== HELPERS ======\n",
    "def norm(x):\n",
    "    if pd.isna(x): return \"\"\n",
    "    return str(x).replace(\"\\u00A0\", \" \").strip().lower()\n",
    "\n",
    "def find_col(cols, candidates):\n",
    "    \"\"\"‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏ö‡∏ö‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏Ñ‡∏™‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå\"\"\"\n",
    "    cols_list = list(cols)\n",
    "    for c in candidates:\n",
    "        if c in cols_list:\n",
    "            return c\n",
    "    lower_map = {c.lower(): c for c in cols_list}\n",
    "    for c in candidates:\n",
    "        if c.lower() in lower_map:\n",
    "            return lower_map[c.lower()]\n",
    "    return None\n",
    "\n",
    "def nonempty_series_like(df, col):\n",
    "    \"\"\"‡∏Ñ‡∏∑‡∏ô Series ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏ã‡∏µ‡∏£‡∏µ‡∏™‡πå‡∏ß‡πà‡∏≤‡∏á) ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á nan/None ‡πÄ‡∏õ‡πá‡∏ô '' \"\"\"\n",
    "    if col is None or col not in df.columns:\n",
    "        return pd.Series(\"\", index=df.index)\n",
    "    return (df[col].astype(str)\n",
    "                 .str.strip()\n",
    "                 .replace({\"nan\": \"\", \"None\": \"\", \"NaT\": \"\"}))\n",
    "\n",
    "def wfo_clean(x):\n",
    "    \"\"\"‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î WFO -> ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö wfo-xxxx (‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å, ‡∏ï‡∏±‡∏î‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÅ‡∏õ‡∏•‡∏Å)\"\"\"\n",
    "    if pd.isna(x): return \"\"\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"none\",\"nan\"}:\n",
    "        return \"\"\n",
    "    s2 = s.lower()\n",
    "    if not s2.startswith(\"wfo-\"):\n",
    "        s2 = \"wfo-\" + s2\n",
    "    s2 = re.sub(r\"[^a-z0-9\\-]\", \"\", s2)\n",
    "    return s2\n",
    "\n",
    "def prefer(left, right):\n",
    "    \"\"\"‡πÄ‡∏•‡∏∑‡∏≠‡∏Å left ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏á ‡∏°‡∏¥‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô‡πÉ‡∏ä‡πâ right\"\"\"\n",
    "    ls = left.astype(str).str.strip()\n",
    "    rs = right.astype(str).str.strip()\n",
    "    return np.where(ls != \"\", ls, rs)\n",
    "\n",
    "def resolve_eff(df, colname, suffix):\n",
    "    \"\"\"‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏•‡∏±‡∏á merge (‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏° suffix ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà)\"\"\"\n",
    "    if not colname:\n",
    "        return None\n",
    "    if f\"{colname}{suffix}\" in df.columns:\n",
    "        return f\"{colname}{suffix}\"\n",
    "    return colname if colname in df.columns else None\n",
    "\n",
    "def _safe_save_csv(df, path):\n",
    "    try:\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"[OK] Saved: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERR] Save failed: {path} -> {e}\")\n",
    "\n",
    "# ====== LOAD ======\n",
    "df_bkf = pd.read_csv(BKF_PATH)\n",
    "df_o7  = pd.read_csv(OUT7_PATH)\n",
    "df_c   = pd.read_csv(CAND_PATH)\n",
    "\n",
    "# ====== IDENTIFY KEY COLUMNS ======\n",
    "bkf_spec = find_col(df_bkf.columns, [\"specific_name\"])\n",
    "bkf_wfo  = find_col(df_bkf.columns, [\"wfo_id\"])\n",
    "bkf_url  = find_col(df_bkf.columns, [\"species_url\"])\n",
    "bkf_full = find_col(df_bkf.columns, [\"wfo_full_name\",\"wfo_name\",\"full_name\",\"accepted_name\"])\n",
    "\n",
    "if bkf_spec is None:\n",
    "    raise KeyError(\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå specific_name ‡πÉ‡∏ô BKF\")\n",
    "\n",
    "if bkf_wfo is None:\n",
    "    df_bkf[\"wfo_id\"] = np.nan\n",
    "    bkf_wfo = \"wfo_id\"\n",
    "\n",
    "o7_spec  = find_col(df_o7.columns, [\"specific_name\"])\n",
    "o7_wfo   = find_col(df_o7.columns, [\"wfo_id\",\"wfo\"])\n",
    "o7_full  = find_col(df_o7.columns, [\"wfo_full_name\"])\n",
    "\n",
    "c_spec   = find_col(df_c.columns, [\"specific_name\"])\n",
    "c_wfo    = find_col(df_c.columns, [\"wfo_id\"])\n",
    "c_full   = find_col(df_c.columns, [\"wfo_full_name\"])\n",
    "\n",
    "# ====== NORMALIZE KEY & WFO ======\n",
    "df_bkf[\"specific_name_clean\"] = df_bkf[bkf_spec].apply(norm)\n",
    "if bkf_url: df_bkf[\"species_url_clean\"] = df_bkf[bkf_url].apply(norm)\n",
    "\n",
    "if o7_spec: df_o7[\"specific_name_clean\"] = df_o7[o7_spec].apply(norm)\n",
    "if c_spec:  df_c[\"specific_name_clean\"]  = df_c[c_spec].apply(norm)\n",
    "\n",
    "df_bkf[bkf_wfo] = nonempty_series_like(df_bkf, bkf_wfo).apply(wfo_clean)\n",
    "if o7_wfo:\n",
    "    df_o7[o7_wfo] = nonempty_series_like(df_o7, o7_wfo).apply(wfo_clean)\n",
    "if c_wfo:\n",
    "    df_c[c_wfo]   = nonempty_series_like(df_c, c_wfo).apply(wfo_clean)\n",
    "\n",
    "# ====== STEP 0: BASELINE ======\n",
    "missing_before = df_bkf[bkf_wfo].eq(\"\") | df_bkf[bkf_wfo].isna()\n",
    "print(f\"[BASELINE] Missing WFO (BKF): {int(missing_before.sum())} / {len(df_bkf)}\")\n",
    "\n",
    "# ====== STEP 1: JOIN output(7) ‡∏î‡πâ‡∏ß‡∏¢ specific_name_clean ======\n",
    "o7_use_cols = [\"specific_name_clean\"]\n",
    "if o7_wfo:  o7_use_cols.append(o7_wfo)\n",
    "if o7_full: o7_use_cols.append(o7_full)\n",
    "df_o7_use = (df_o7[o7_use_cols].drop_duplicates(\"specific_name_clean\")\n",
    "             if o7_spec else pd.DataFrame(columns=o7_use_cols))\n",
    "\n",
    "df1 = df_bkf.merge(df_o7_use, on=\"specific_name_clean\", how=\"left\", suffixes=(\"\",\"_o7\"))\n",
    "\n",
    "# ====== STEP 2: PREP candidates ‚Äî ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ WFO ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ======\n",
    "if (c_spec is not None) and (c_wfo is not None):\n",
    "    cg = df_c.dropna(subset=[c_wfo]).copy()\n",
    "    cg[c_wfo] = cg[c_wfo].astype(str).str.strip()\n",
    "    uniq_one_key = (\n",
    "        cg.groupby(\"specific_name_clean\")[c_wfo].nunique()\n",
    "          .reset_index(name=\"n\").query(\"n == 1\")[[\"specific_name_clean\"]]\n",
    "    )\n",
    "    c_use = (\n",
    "        cg.merge(uniq_one_key, on=\"specific_name_clean\", how=\"inner\")\n",
    "          .drop_duplicates(subset=[\"specific_name_clean\"])\n",
    "    )\n",
    "    c_use_cols = [\"specific_name_clean\", c_wfo]\n",
    "    if c_full: c_use_cols.append(c_full)\n",
    "    c_use = c_use[c_use_cols]\n",
    "else:\n",
    "    c_use = pd.DataFrame(columns=[\"specific_name_clean\"])\n",
    "\n",
    "df2 = df1.merge(c_use, on=\"specific_name_clean\", how=\"left\", suffixes=(\"\",\"_cand\"))\n",
    "\n",
    "# ====== STEP 2.1: RESOLVE EFFECTIVE NAMES ‡∏´‡∏•‡∏±‡∏á merge ======\n",
    "o7_wfo_eff  = resolve_eff(df2, o7_wfo,  \"_o7\")\n",
    "o7_full_eff = resolve_eff(df2, o7_full, \"_o7\")\n",
    "c_wfo_eff   = resolve_eff(df2, c_wfo,   \"_cand\")\n",
    "c_full_eff  = resolve_eff(df2, c_full,  \"_cand\")\n",
    "\n",
    "# ====== STEP 3: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å wfo_id ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ======\n",
    "bkf_val = nonempty_series_like(df2, bkf_wfo)\n",
    "o7_val  = nonempty_series_like(df2, o7_wfo_eff) if o7_wfo_eff else pd.Series(\"\", index=df2.index)\n",
    "c_val   = nonempty_series_like(df2, c_wfo_eff)  if c_wfo_eff  else pd.Series(\"\", index=df2.index)\n",
    "\n",
    "bkf_has = bkf_val != \"\"\n",
    "o7_has  = o7_val  != \"\"\n",
    "c_has   = c_val   != \"\"\n",
    "\n",
    "df2[\"wfo_id_final\"] = np.select(\n",
    "    [bkf_has, (~bkf_has) & o7_has, (~bkf_has) & (~o7_has) & c_has],\n",
    "    [bkf_val, o7_val, c_val],\n",
    "    default=\"\"\n",
    ")\n",
    "\n",
    "df2[\"wfo_source\"] = np.select(\n",
    "    [bkf_has, (~bkf_has) & o7_has, (~bkf_has) & (~o7_has) & c_has],\n",
    "    [\"bkf_original\",\"output7_exact\",\"candidates_unique\"],\n",
    "    default=\"unfilled\"\n",
    ")\n",
    "\n",
    "# ====== STEP 3.1: FLAG CONFLICTS ======\n",
    "def diff_nonempty(a, b):\n",
    "    a = a.astype(str).str.strip(); b = b.astype(str).str.strip()\n",
    "    return (a != \"\") & (b != \"\") & (a != b)\n",
    "\n",
    "df2[\"conflict_bkf_vs_o7\"]   = diff_nonempty(bkf_val, o7_val)\n",
    "df2[\"conflict_bkf_vs_cand\"] = diff_nonempty(bkf_val, c_val)\n",
    "df2[\"conflict_o7_vs_cand\"]  = diff_nonempty(o7_val, c_val)\n",
    "df2[\"any_conflict\"] = df2[[\"conflict_bkf_vs_o7\",\"conflict_bkf_vs_cand\",\"conflict_o7_vs_cand\"]].any(axis=1)\n",
    "\n",
    "# ====== STEP 4: wfo_full_name ‡∏à‡∏≤‡∏Å output(7) ======\n",
    "if o7_wfo and o7_full:\n",
    "    o7_id_name = (\n",
    "        df_o7[[o7_wfo, o7_full]]\n",
    "        .dropna(subset=[o7_wfo])\n",
    "        .copy()\n",
    "    )\n",
    "    o7_id_name[o7_wfo]  = o7_id_name[o7_wfo].astype(str).str.strip()\n",
    "    o7_id_name[o7_full] = o7_id_name[o7_full].astype(str).str.strip()\n",
    "    o7_id_name = (\n",
    "        o7_id_name[o7_id_name[o7_wfo] != \"\"]\n",
    "        .drop_duplicates(subset=[o7_wfo], keep=\"first\")\n",
    "    )\n",
    "    o7_map = dict(zip(o7_id_name[o7_wfo], o7_id_name[o7_full]))\n",
    "    df2[\"wfo_full_name_final\"] = df2[\"wfo_id_final\"].map(o7_map).fillna(\"\")\n",
    "else:\n",
    "    df2[\"wfo_full_name_final\"] = \"\"\n",
    "\n",
    "# ====== STEP 5: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô ======\n",
    "df_out = df2.copy()\n",
    "df_out[\"wfo_id\"] = df_out[\"wfo_id_final\"].replace({\"\": np.nan})\n",
    "df_out[\"wfo_full_name\"] = df_out[\"wfo_full_name_final\"].replace({\"\": np.nan})\n",
    "\n",
    "# ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏±‡∏Å)\n",
    "drop_cols = []\n",
    "for col in [\n",
    "    o7_wfo_eff, o7_full_eff, c_wfo_eff, c_full_eff,\n",
    "    \"wfo_id_final\", \"wfo_full_name_final\",\n",
    "    \"specific_name_clean\", \"species_url_clean\",\n",
    "    \"conflict_bkf_vs_o7\",\"conflict_bkf_vs_cand\",\"conflict_o7_vs_cand\",\"any_conflict\"\n",
    "]:\n",
    "    if col and (col in df_out.columns) and (col not in [\"wfo_id\",\"wfo_full_name\"]):\n",
    "        drop_cols.append(col)\n",
    "df_out = df_out.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "# ====== STEP 6: SAVE (Organized) ======\n",
    "enriched_path = ENRICH_DIR / \"bkf_wfo_enriched.csv\"\n",
    "unmatched = df_out[df_out[\"wfo_id\"].isna() | (df_out[\"wfo_id\"].astype(str).str.strip()==\"\")]\n",
    "unmatched_path = ENRICH_DIR / \"bkf_wfo_unmatched.csv\"\n",
    "\n",
    "_safe_save_csv(df_out, enriched_path)\n",
    "_safe_save_csv(unmatched, unmatched_path)\n",
    "\n",
    "# ====== STEP 7: AUDIT / DIAGNOSTIC ======\n",
    "audit_cols = [bkf_spec, bkf_wfo, \"specific_name_clean\"]\n",
    "if bkf_full: audit_cols.append(bkf_full)\n",
    "if o7_wfo_eff:  audit_cols.append(o7_wfo_eff)\n",
    "if o7_full_eff: audit_cols.append(o7_full_eff)\n",
    "if c_wfo_eff:   audit_cols.append(c_wfo_eff)\n",
    "if c_full_eff:  audit_cols.append(c_full_eff)\n",
    "audit_cols = [c for i,c in enumerate(audit_cols) if c and (audit_cols.index(c) == i)]\n",
    "\n",
    "df_audit = df2[audit_cols + [\n",
    "    \"wfo_id_final\",\"wfo_source\",\n",
    "    \"conflict_bkf_vs_o7\",\"conflict_bkf_vs_cand\",\"conflict_o7_vs_cand\",\"any_conflict\",\n",
    "    \"wfo_full_name_final\"\n",
    "]].copy()\n",
    "\n",
    "rename_map = {}\n",
    "if bkf_spec: rename_map[bkf_spec] = \"bkf_specific_name\"\n",
    "if bkf_wfo:  rename_map[bkf_wfo]  = \"bkf_wfo_id\"\n",
    "if o7_wfo_eff:  rename_map[o7_wfo_eff]  = \"o7_wfo_id\"\n",
    "if c_wfo_eff:   rename_map[c_wfo_eff]   = \"cand_wfo_id\"\n",
    "if bkf_full:    rename_map[bkf_full]    = \"bkf_wfo_full_name\"\n",
    "if o7_full_eff: rename_map[o7_full_eff] = \"o7_wfo_full_name\"\n",
    "if c_full_eff:  rename_map[c_full_eff]  = \"cand_wfo_full_name\"\n",
    "rename_map[\"wfo_full_name_final\"] = \"wfo_full_name_final\"\n",
    "\n",
    "df_audit.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "audit_conflicts_path  = AUDIT_DIR / \"bkf_wfo_audit_conflicts.csv\"\n",
    "_safe_save_csv(df_audit, audit_conflicts_path)\n",
    "\n",
    "if \"specific_name_clean\" in df2.columns:\n",
    "    dup_name = (df2.assign(_wfo=df2[\"wfo_id_final\"].replace({\"\": np.nan}))\n",
    "                   .dropna(subset=[\"_wfo\"])\n",
    "                   .groupby(\"specific_name_clean\")[\"_wfo\"].nunique()\n",
    "                   .reset_index().query(\"_wfo > 1\"))\n",
    "    audit_name_multi_path = AUDIT_DIR / \"audit_name_to_multiple_wfo.csv\"\n",
    "    _safe_save_csv(dup_name, audit_name_multi_path)\n",
    "else:\n",
    "    dup_name = None\n",
    "\n",
    "wfo_counts = (df2.assign(_wfo=df2[\"wfo_id_final\"].replace({\"\": np.nan}))\n",
    "                .dropna(subset=[\"_wfo\"])\n",
    "                .groupby(\"_wfo\")[\"specific_name_clean\"].nunique()\n",
    "                .reset_index(name=\"n_names\").sort_values(\"n_names\", ascending=False))\n",
    "audit_wfo_multi_path  = AUDIT_DIR / \"audit_wfo_to_multiple_names.csv\"\n",
    "_safe_save_csv(wfo_counts, audit_wfo_multi_path)\n",
    "\n",
    "# ====== STEP 8: SUMMARY ======\n",
    "total = len(df_bkf)\n",
    "filled = total - len(unmatched)\n",
    "src_counts = Counter(df2[\"wfo_source\"])\n",
    "conflicts = int(df2[\"any_conflict\"].sum()) if \"any_conflict\" in df2.columns else None\n",
    "\n",
    "print(\"\\n=== COVERAGE & SOURCE ===\")\n",
    "print(f\"Total BKF rows: {total}\")\n",
    "print(f\"Filled (any source): {filled}  |  Coverage: {filled/total:.1%}\")\n",
    "for k in [\"bkf_original\",\"output7_exact\",\"candidates_unique\",\"unfilled\"]:\n",
    "    print(f\"  {k:18s}: {src_counts.get(k,0)}\")\n",
    "print(f\"Any conflicts flagged: {conflicts}\")\n",
    "\n",
    "summary = {\n",
    "    \"total_bkf_rows\": total,\n",
    "    \"filled_rows\": filled,\n",
    "    \"coverage_ratio\": round(filled/total, 4),\n",
    "    \"source_breakdown\": {str(k): int(v) for k,v in src_counts.items()},\n",
    "    \"any_conflicts\": conflicts,\n",
    "    \"run_stamp\": RUN_STAMP,\n",
    "}\n",
    "summary_json = SUMMARY_DIR / \"coverage_summary.json\"\n",
    "with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[OK] Saved summary: {summary_json}\")\n",
    "\n",
    "summary_csv = SUMMARY_DIR / \"coverage_summary.csv\"\n",
    "pd.DataFrame([summary]).to_csv(summary_csv, index=False)\n",
    "print(f\"[OK] Saved summary CSV: {summary_csv}\")\n",
    "\n",
    "# ====== STEP 9: SNAPSHOT (copy to runs/<stamp>) ======\n",
    "for p in [\n",
    "    enriched_path, unmatched_path,\n",
    "    audit_conflicts_path, audit_wfo_multi_path\n",
    "]:\n",
    "    if p.exists():\n",
    "        dest = RUN_DIR / p.name\n",
    "        shutil.copy2(p, dest)\n",
    "        print(f\"[SNAPSHOT] {p.name} -> {dest}\")\n",
    "if 'audit_name_multi_path' in locals():\n",
    "    if audit_name_multi_path.exists():\n",
    "        shutil.copy2(audit_name_multi_path, RUN_DIR / audit_name_multi_path.name)\n",
    "        print(f\"[SNAPSHOT] {audit_name_multi_path.name} -> {RUN_DIR / audit_name_multi_path.name}\")\n",
    "for p in [summary_json, summary_csv]:\n",
    "    if p.exists():\n",
    "        shutil.copy2(p, RUN_DIR / p.name)\n",
    "        print(f\"[SNAPSHOT] {p.name} -> {RUN_DIR / p.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f52f3",
   "metadata": {},
   "source": [
    "\n",
    "### üìù ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
    "- **CONFIG:** ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏û‡∏≤‡∏ò `ROOT_DIR`, ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå input (`Raw data`) ‡πÅ‡∏•‡∏∞ output (`Processed data`)  \n",
    "- **HELPERS:** ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞ WFO (`norm`, `wfo_clean`) ‡πÅ‡∏•‡∏∞‡∏¢‡∏π‡∏ó‡∏¥‡∏•‡∏¥‡∏ï‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ  \n",
    "- **STEP 0‚Äì3:** ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å BKF + output(7) + candidates ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à `wfo_id_final` ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç  \n",
    "- **STEP 4:** ‡πÄ‡∏ï‡∏¥‡∏° `wfo_full_name` ‡∏à‡∏≤‡∏Å mapping ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå output(7)  \n",
    "- **STEP 5:** ‡∏à‡∏±‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô (`wfo_id`, `wfo_full_name`)  \n",
    "- **STEP 6:** ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å **enriched** + **unmatched** ‡πÑ‡∏õ‡∏ó‡∏µ‡πà `01_enriched/`  \n",
    "- **STEP 7:** ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå **audit** (conflicts, name‚ÜîWFO, WFO‚Üînames) ‡πÑ‡∏õ‡∏ó‡∏µ‡πà `02_audit/`  \n",
    "- **STEP 8:** ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏£‡∏∏‡∏õ **summary (JSON/CSV)** ‡πÑ‡∏õ‡∏ó‡∏µ‡πà `03_summary/`  \n",
    "- **STEP 9:** ‡∏ó‡∏≥‡∏™‡∏≥‡πÄ‡∏ô‡∏≤ snapshot ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏õ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà `runs/<timestamp>/`  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
